{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tic_tac_toe import TicTacToe\n",
    "from negamax import NegaMax\n",
    "from game_tic_tac_toe import Game\n",
    "from human_ui import HumanUI\n",
    "from board_position_state import BoardPositionState\n",
    "from board_position_states import BoardPositionStates\n",
    "from reinforcement_learning import ReinforcementLearning\n",
    "from trainer import Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Player interface\n",
    "\n",
    "The game player interface is described in player.py\n",
    "\n",
    "(This is more for documentation purposes, as none of the methods are implemented.)\n",
    "\n",
    "It basically has two functions:\n",
    "- get_move (board, possible_moves, player_1_or_2)\n",
    "- update_probabilities_as_game_is_over(final_result)   # This is only used by the Reinforcement Learning algorithm\n",
    "\n",
    "All player classes will implement these two functions.\n",
    "\n",
    "This is how the Game class interacts with the two players.\n",
    "\n",
    "A player can be a\n",
    "- UI (to allow the user to play)\n",
    "- NegaMax (brute force algorithm)\n",
    "- Reinforcement Learning algorithm (which learns by playing NegaMax)\n",
    "\n",
    "The Game class has a Start method, which takes two player objects as parameters.\n",
    "\n",
    "It then calls get_move for each player in turn, updating the game board, after each player's move."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NegaMax vs User\n",
    "\n",
    "NegaMax defaults to a depth of 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Game().start(HumanUI(),NegaMax())\n",
    "# UI play sequence for demo (NegaMax blocks opponent and wins)\n",
    "#   2    3    3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UI play sequence for demo (NegaMax blocks opponent and wins)\n",
    "#   2    3    3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### User vs Reinforcement Learning algorithm\n",
    "\n",
    "The RL algorithm needs to know which player it is when it is instantiated i.e. 1 or -1 (player 1 or 2)\n",
    "\n",
    "Then it must be passed in as the corresponding first or second parameter to the Game start method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prints out the Reinforcement Learning player's learned position probabilities in the given order\n",
    "from collections import OrderedDict\n",
    "\n",
    "def print_rl_position_probabilities(player_rl, order_is_reverse=False):\n",
    "    \"\"\"Prints out the Reinforcement Learning player's learned position probabilities in the given order\"\"\"\n",
    "    \n",
    "    position_key_values = player_rl.board_position_states.position_states_dictionary.items()\n",
    "    position_key_values_sorted = sorted(position_key_values,key=lambda x: x[1].get_win_draw_prob_value(),\\\n",
    "                                        reverse=order_is_reverse)\n",
    "    position_key_values_sorted_dict = OrderedDict(position_key_values_sorted)\n",
    "    # d_sorted_by_value = OrderedDict(sorted(d.items(), key=lambda x: x[1]))\n",
    "    \n",
    "    print('Board probabilities:')\n",
    "    print()\n",
    "\n",
    "    # for board_key, board_position_state in player1.board_position_states.position_states_dictionary.items():\n",
    "    for board_key, board_position_state in position_key_values_sorted_dict.items():\n",
    "#         print('board_key:', np.asarray(board_key))\n",
    "        print('board_key:')\n",
    "        print(np.asarray(board_key)) # convert nested tuples to nested arrays for better printing\n",
    "        print('win_probability:', board_position_state.win_probability)\n",
    "        print('draw_probability:', board_position_state.draw_probability)\n",
    "        print('win_draw_prob:', board_position_state.get_win_draw_prob_value())\n",
    "        print()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "player2_rl = ReinforcementLearning(-1) # initialize new (untrained) RL algorithm as player2 (= -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Game().start(HumanUI(), player2_rl)\n",
    "# Game sequence for demo\n",
    "#   4     0[unless he went bottom right]    [pick position to win: usually -1 (or -2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Game sequence for demo\n",
    "#   4     0[unless he went bottom right]    [pick position to win: usually -1 (or -2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_rl_position_probabilities(player2_rl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "player1_rl = ReinforcementLearning(1)  # initialize new (untrained) RL algorithm\n",
    "player2_nm = NegaMax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Trainer().start(player1_rl, player2_nm, 1_000)\n",
    "# Timing\n",
    "# 1_000 = 1,000 games: 14 seconds approx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(player1_rl.win_draw_loss_record, columns=[\"win_loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_by_100(df, win_loss):\n",
    "    \"\"\"Groups all wins or losses (win_loss filter is 1 or -1) in the dataframe \n",
    "    and returns a new dataframe\"\"\"\n",
    "    \n",
    "    group_size = 100\n",
    "    df_filtered = df[df.win_loss == win_loss]\n",
    "    \n",
    "    # use absolute (abs) in case we are summing losses i.e. negative one (-1)\n",
    "    return abs(df_filtered.groupby((df_filtered.index//group_size)*group_size).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_win_100 = group_by_100(df, 1)\n",
    "#df_win_100\n",
    "df_loss_100 = group_by_100(df, -1)\n",
    "#df_loss_100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_win_losses(df_win, df_loss, number_of_games):\n",
    "    plt.title(f'Win Loss rate for Reinforcement Algorithm \\\n",
    "per 100 games played. Learning:0.2. Depth:2. {str(number_of_games)} games')\n",
    "\n",
    "    plt.xlabel('Total number of games played (Draws not shown above)')\n",
    "    plt.ylabel('Wins(Green) Losses(Red)')\n",
    "    plt.plot(df_win, c='green') \n",
    "    plt.plot(df_loss, c='red')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_win_losses(df_win_100, df_loss_100, '1,000')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To save time, a previously executed 10,000 game Reinforcement Learning algorithm dataframe has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_10_000_disk = pd.read_json('df_10_000.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_10_000_disk.win_loss.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_win_100_disk = group_by_100(df_10_000_disk, 1)\n",
    "df_loss_100_disk = group_by_100(df_10_000_disk, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_win_losses(df_win_100_disk, df_loss_100_disk, '1,000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AWS (Amazon Web Services) Deep Racer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### AWS Deep Racer allows you to traing a Reinforcement Learning algorithm to drive a car\n",
    "\n",
    "The car is trained on a virtual race track running on Amazon's cloud infrastructure.\n",
    "The model can then be submitted to virtual races where your time is compared to other competitors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### There is also a physical 1/18th scale car that you can buy and download your trained model into, and then compete in actual (physical) races run by Amazon\n",
    "\n",
    "https://reinvent.awsevents.com/learn/deepracer/\n",
    "\n",
    "https://www.amazon.com/AWS-DeepRacer-Fully-autonomous-developers/dp/B07JMHRKQG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reinforcement Learning: An Introduction,   2nd Edition - Richard S. Sutton and Andrew G. Barto\n",
    "\n",
    "Online PDF: http://incompleteideas.net/book/bookdraft2017nov5.pdf\n",
    "\n",
    "Amazon: https://www.amazon.com/Reinforcement-Learning-Introduction-Adaptive-Computation/dp/0262039249/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Applications of Reinforcement Learning\n",
    "\n",
    "\n",
    "#### Robot uses Reinforcement Learning to train itself in picking up - MIT Technology Review - Fanuc robot\n",
    "\n",
    "https://www.technologyreview.com/s/601045/this-factory-robot-learns-a-new-job-overnight/\n",
    "\n",
    "\n",
    "#### DeepMind AI Reduces Google Data Centre Cooling Bill by 40%\n",
    "\n",
    "https://deepmind.com/blog/article/deepmind-ai-reduces-google-data-centre-cooling-bill-40\n",
    "\n",
    "\n",
    "#### A Deep Reinforcement Learning Approach to Autonomous Driving\n",
    "\n",
    "https://web.stanford.edu/~anayebi/projects/CS_239_Final_Project_Writeup.pdf\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### AlphaGo Zero - DeepMind blog\n",
    "\n",
    "https://deepmind.com/blog/article/alphago-zero-starting-scratch\n",
    "\n",
    "\n",
    "#### Playing Atari with Deep Reinforcement Learning - DeepMind Technologies\n",
    "\n",
    "https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On Choosing a Deep Reinforcement Learning Library\n",
    "\n",
    "https://medium.com/data-from-the-trenches/choosing-a-deep-reinforcement-learning-library-890fb0307092"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advanced Reinforcement Learning by Udacity :- https://www.udacity.com/course/reinforcement-learning--ud600"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Reinforcement Learning CS 285 BY UC Berkeley :-  http://rail.eecs.berkeley.edu/deeprlcourse/?fbclid=IwAR3sqqeNMtClYv2wsUqGEiiJrXRn0VhcJTY49GkdsumvFTzeTNJRX5Hev9s    \n",
    "Complete Playlist :- https://www.youtube.com/watch?v=zVA5Pg8FVW4&list=PLkFD6_40KJIxJMR-j5A1mkxK26gh_qg37&index=2&t=0s (!?!?!?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deep RL Bootcamp - Berkeley CA\n",
    "\n",
    "https://sites.google.com/view/deep-rl-bootcamp/lectures "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Amazon AWS DeepRacer\n",
    "\n",
    "https://medium.com/@banerjee.siddhartha.sb/aws-deepracer-looking-under-the-hood-for-design-of-the-reward-function-and-adjusting-e9dd3805ebbf  \n",
    "\n",
    "The log-analysis and local training - blogs :- https://codelikeamother.uk/using-jupyter-notebook-for-analysing-deepracer-s-logs https://github.com/aws-samples/aws-deepracer-workshops/blob/master/log-analysis/DeepRacer%20Log%20Analysis.ipynb  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://deepsense.ai/what-is-reinforcement-learning-the-complete-guide/  A complete guide for Reinforcement Learning for beginners."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The role of the Discount Factor on Reinforcement Learning :https://stats.stackexchange.com/questions/221402/understanding-the-role-of-the-discount-factor-in-reinforcement-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reinforcement learning tutorial with TensorFlow:\n",
    "\n",
    "https://adventuresinmachinelearning.com/reinforcement-learning-tensorflow/ (edited) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "https://www.youtube.com/playlist?list=PLdAoL1zKcqTXFJniO3Tqqn6xMBBL07EDc  Basic Reinforcement Learning By University of Waterloo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uses RL to solve a rubiks cube:  https://openai.com/blog/solving-rubiks-cube/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi-Agent-Reinforcement Learning by Microsoft:- https://www.youtube.com/watch?v=Yd6HNZnqjis&list=PLD7HFcN7LXRe9nWEX3Up-RiCDi6-0mqVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Deep Q Network vs Policy Gradients\n",
    "\n",
    "https://flyyufelix.github.io/2017/10/12/dqn-vs-pg.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Demystifying Deep Reinforcement Learning\n",
    "\n",
    "https://www.intel.ai/demystifying-deep-reinforcement-learning/#gs.gazc94"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Introduction to Various Reinforcement Learning Algorithms\n",
    "\n",
    "https://towardsdatascience.com/introduction-to-various-reinforcement-learning-algorithms-part-ii-trpo-ppo-87f2c5919bb9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \tFree Udacity course:  Intro to TensorFlow for Deep Learning\n",
    "https://www.udacity.com/course/intro-to-tensorflow-for-deep-learning--ud187"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introduction to Deep Reinforcement Learning and Control - Carnegie Mellon\n",
    "\n",
    "https://www.cs.cmu.edu/~katef/DeepRLFall2018/lecture1_intro.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### YouTube Recommendation System uses Reinforcement Learning - NY Times article - search for Reinforce\n",
    "\n",
    "https://www.nytimes.com/interactive/2019/06/08/technology/youtube-radical.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
